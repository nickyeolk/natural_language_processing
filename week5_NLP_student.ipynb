{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import src.week5_func as wk5\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: How do we turn text into data we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your corpus into bags of words\n",
    "\n",
    "We can't apply any of the techniques we have learned over the past few weeks directly on raw text.  Therefore, our first task is to convert our corpus into numbers.  The simplest way to do this is to use a **bag of words**. You can see some examples of this here: https://liferay.de.dariah.eu/tatom/index.html.\n",
    "\n",
    "Once you understand the concept, convert your corpus and documents into bags of words below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence) this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned  \n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "vocabulary = tokenize_sentences(df_raw['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74891\n",
      "[115. 364. 124.  99.  97. 145.  88. 296. 374. 282.]\n"
     ]
    }
   ],
   "source": [
    "n_words = len(vocabulary)\n",
    "print(n_words)\n",
    "n_docs = len(df_raw.iloc[:10])\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "for ii in range(n_docs):\n",
    "    bag_o[ii,:] = bagofwords(df_raw['text'].iloc[ii],vocabulary)\n",
    "print(np.sum(bag_o,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Trying to do it manually causes a memory error. Probably will have to use sparse matrices instead. Let's just implement this with sklearn which does the sparse matrix for you.</font>\n",
    "## <font color='blue'><strong>Try using sklearn instead</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "print(np.shape(train_data_features))\n",
    "# bag_sk  = vectorizer.transform(df_raw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame({'word':vectorizer.get_feature_names() ,'freq':np.array(np.sum(train_data_features,axis=0)).flatten()})\n",
    "df_freq.sort_values('freq',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "Counter(word_tokenize(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags\n",
    "\n",
    "Show and explain what one of your documents looks like as a bag of words below.  What are the advantages and disadvantages of encoding text as bags of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell us a story with your bags\n",
    "Now that your text is in a more digestible format, you can apply previously learned techniques to better understand the corpus. **Create a brief story around your corpus, for example by using clustering techniques.** Some examples of what you can do below:\n",
    "* Use *Hierarchical Clustering* to understand similarity of documents in your corpus. What distance measure works best? Are the results what you expect?\n",
    "* Learn about *Latent Dirichlet Allocation* to extract topics from your corpora, and measure each document on how much of each topic it contains. How do you interpret these topics?\n",
    "\n",
    "Some **potential inspiration** below (but please keep your own story simple!):\n",
    "* https://liferay.de.dariah.eu/tatom/topic_model_mallet.html covers a few examples of text analysis\n",
    "* http://fantheory.viacom.com/\n",
    "* https://pudding.cool/2017/02/vocabulary/\n",
    "\n",
    "Additional resources on LDA (if you are interested): \n",
    "* https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "* https://www.youtube.com/watch?v=DDq3OVp9dNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize your bags\n",
    "In the above exercise, you may find it important to normalize your data.  One useful method when dealing with text is *Term Frequency - Inverse Document Frequency (TF-IDF)*. You can see more detail on this here: http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/.\n",
    "\n",
    "Once you understand the concept, **express your data as TF-IDF vectors (instead of simple bag-of-words counts), and see if it changes your above story**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 2 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 1 0 1 0 1 1 0]]\n",
      "{'am': 0, 'boy': 2, 'girl': 4, 'you': 9, 'are': 1, 'neither': 6, 'what': 8, 'is': 5, 'this': 7, 'cake': 3}\n"
     ]
    }
   ],
   "source": [
    "corpse_us = ['I am a boy','I am a girl girl','you are neither','what is this cake']\n",
    "print(vectorizer.fit_transform(corpse_us).toarray())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfTransformer().fit_transform(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2956225 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags (Version 2)\n",
    "\n",
    "Show and explain what one of your documents looks like as a TF-IDF vector below.  How is this different from a simple bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Supervised Learning with Text\n",
    "Now that you are comfortable with treating text as numbers, we can try out supervised learning.  We'll use a labelled dataset of IMDB reviews to classify each review as 'positive' or 'negative'.  You can **find the data below:**\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Load in and process the data, then train a supervised learning model.  **You should achieve val or test set accuracy of 85%**. Pretty good for a simple bag, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imported and download data using  \n",
    "``` wk5.dl_and_unzip('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')```  \n",
    "Tar file removed to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processed and saved with week5_func.save_as_pkl()\n",
    "df_raw = pd.read_pickle('./data/df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('./data/df_raw_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])\n",
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)\n",
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2956225 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likkhian\\Anaconda3\\envs\\py3env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    return rf\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Playing with Recurrent Neural Networks (RNN)\n",
    "So far, we've only treated text as a simple bag, with reasonable results.  We'll now shift to a more complex representation of language: recurrent neural networks.  To do so, we need to process text at the word or character level, and capture the sequence of a document. \n",
    "\n",
    "Our task here is to build an RNN that 'eats up' sequences of characters in order to predict the next character in a sequence, for every step in the sequence of a document. This is a common (and fun) task, with lots of examples available online. \n",
    "\n",
    "For this task, use existing RNN APIs (don't code everything from scratch) from Keras or PyTorch. \n",
    "\n",
    "**Read up on RNNs and this exercise** below:\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - start here!\n",
    "* https://github.com/martin-gorner/tensorflow-rnn-shakespeare - video, slides and code going through an example with Shakespeare\n",
    "* http://killianlevacher.github.io/blog/posts/post-2016-03-01/post.html - another nice example based on Trump tweets\n",
    "\n",
    "### Prepare your data\n",
    "\n",
    "Our first step is to prepare our text. **Process your corpora into a format that can be used by an RNN, and walkthough one sequence below**.\n",
    "\n",
    "An **example way to shape your data** for this task is as follows (feel free to play around with different structures):\n",
    "\n",
    "*In this example your corpora starts with the string 'the cat and I'*\n",
    "* RNN input: divide your text into sequences of 10 characters e.g. 'the cat an'\n",
    "* RNN output: the 1 character immediately following RNN input sequences e.g. 'd'. \n",
    "* Note: You may or may not want to divide your text into overlapping strings (e.g. RNN input contains 'the cat an', 'he cat and', 'e cat and ', ...) . How is the model different in each case?\n",
    "* Note: Your 'vocabulary' or `vocab_size` here is the number of unique characters in your text (and therefore the number of classes you want to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "converted entire corpus into one long string, then divided string into segments of 10 (or 20) characters. Used input of characters to predict next character. Resulted in some sort of exploding/vanishing gradient problem. outputs turned into NaN very quickly (in ~6-10 iterations).  \n",
    "  \n",
    "Next I'm goning to try one-hot encoding the inputs.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead).This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.Yes its an art... to successfully make a slow paced thriller.<br /><br />The story unfolds in nice volumes while you don't even notice it happening.<br /><br />Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of suspense and unexpectedness is good.<br /><br />Very nice 1 time watch if you are looking to lay back and hear a thrilling short story!In this \"critically acclaimed psychological thriller based on true events, Gabriel (Robin Williams), a celebrated writer and late-night talk show host, becomes captivated by the harrowing story of a young listener and his adoptive mother (Toni Collette). When troubling questions arise about this boy's (story), however, Gabriel finds himself drawn into a widening mystery that hides a deadly secret",
      "\" according to film's official synopsis.<br /><br />You really should STOP reading these comments, and watch the film NOW...<br /><br />The \"How did he lose his leg?\" ending, with Ms. Collette planning her new life, should be chopped off, and sent to \"deleted scenes\" land. It's overkill. The true nature of her physical and mental ailments should be obvious, by the time Mr. Williams returns to New York. Possibly, her blindness could be in question - but a revelation could have be made certain in either the \"highway\" or \"video tape\" scenes. The film would benefit from a re-editing - how about a \"director's cut\"? <br /><br />Williams and Bobby Cannavale (as Jess) don't seem, initially, believable as a couple. A scene or two establishing their relationship might have helped set the stage. Otherwise, the cast is exemplary. Williams offers an exceptionally strong characterization, and not a \"gay impersonation\". Sandra Oh (as Anna), Joe Morton (as Ashe), and Rory Culkin (Pete Logand) are all perfect.<br /><br />Best of all, Collette's \"Donna\" belongs in the creepy hall of fame. Ms. Oh is correct in saying Collette might be, \"you know, like that guy from 'Psycho'.\" There have been several years when organizations giving acting awards seemed to reach for women, due to a slighter dispersion of roles; certainly, they could have noticed Collette with some award consideration. She is that good. And, director Patrick Stettner definitely evokes Hitchcock - he even makes getting a sandwich from a vending machine suspenseful.<br /><br />Finally, writers Stettner, Armistead Maupin, and Terry Anderson deserve gratitude from flight attendants everywhere.<br /><br />******* The Night Listener (1/21/06) Patrick Stettner ~ Robin Williams, Toni Collette, Sandra Oh, Rory CulkinTHE NIGHT LISTENER (2006) **1/2 Robin Williams, Toni Collette, Bobby Cannavale, Rory Culkin, Joe Morton, Sandra Oh, John Cullum, Lisa Emery, Becky Ann Baker. (Dir: Patrick Stettner) <br /><br />Hitchcockian suspenser gives Williams a stand-out low-key performance.<br /><br />What is it about celebrities and fans? What is the near paranoia one associates with the other and why is it almost the norm? <br /><br />In the latest derange fan scenario, based on true events no less, Williams stars as a talk-radio personality named Gabriel No one, who reads stories he's penned over the airwaves and has accumulated an interesting fan in the form of a young boy named Pete Logand (Culkin) who has submitted a manuscript about the travails of his troubled youth to No one's editor Ashe (Morton) who gives it to No one to read for himself. <br /><br />No one is naturally disturbed but ultimately intrigued about the nightmarish existence of Pete being abducted and sexually abused for years until he was finally rescued by a nurse named Donna (Collette giving an excellent performance) who has adopted the boy but her correspondence with No one reveals that Pete is dying from AIDS. Naturally No one wants to meet the fans but is suddenly in doubt to their possibly devious ulterior motives when the seed is planted by his estranged lover Jess (Cannavale) whose sudden departure from their New York City apartment has No one in an emotional tailspin that has only now grown into a tempest in a teacup when he decides to do some investigating into Donna and Pete's backgrounds discovering some truths that he didn't anticipate.<br /><br />Written by Armistead Maupin (who co-wrote the screenplay with his former lover Terry Anderson and the film's novice director Stettner) and based on a true story about a fan's hoax found out has some Hitchcockian moments that run on full tilt like any good old fashioned pot-boiler does. It helps that Williams gives a stand-out, low-key performance as the conflicted good-hearted personality who genuinely wants to believe that his number one fan is in fact real and does love him (the one thing that has escaped his own reality) and has some unsettling dreadful moments with the creepy Collette whose one physical trait I will leave unmentioned but underlines the desperation of her character that can rattle you to the core.<br /><br />However the film runs out of gas and eventually becomes a bit repetitive and predictable despite a finely directed piece of hoodwink and mystery by Stettner, it pays to listen to your own inner voice: be careful of what you hope for.You know, Robin Williams, God bless him, is constantly shooting himself in the foot lately with all these dumb comedies he has done this decade (with perhaps the exception of \"Death To Smoochy\", which bombed when it came out but is now a cult classic). The dramas he has made lately have been fantastic, especially \"Insomnia\" and \"One Hour Photo\". \"The Night Listener\", despite mediocre reviews and a quick DVD release, is among his best work, period.<br /><br />This is a very chilling story, even though it doesn't include a serial killer or anyone that physically dangerous for that matter. The concept of the film is based on an actual case of fraud that still has yet to be officially confirmed. In high school, I read an autobiography by a child named Anthony Godby Johnson, who suffered horrific abuse and eventually contracted AIDS as a result. I was moved by the story until I read reports online that Johnson may not actually exist. When I saw this movie, the confused feelings that Robin Williams so brilliantly portrayed resurfaced in my mind.<br /><br />Toni Collette probably gives her best dramatic performance too as the ultimately sociopathic \"caretaker\". Her role was a far cry from those she had in movies like \"Little Miss Sunshine\". There were even times she looked into the camera where I thought she was staring right at me. It takes a good actress to play that sort of role, and it's this understated (yet well reviewed) role that makes Toni Collette probably one of the best actresses of this generation not to have even been nominated for an Academy Award (as of 2008). It's incredible that there is at least one woman in this world who is like this, and it's scary too.<br /><br />This is a good, dark film that I highly recommend. Be prepared to be unsettled, though, because this movie leaves you with a strange feeling at the end.\n"
     ]
    }
   ],
   "source": [
    "# lets start by predicting just positive stuff.\n",
    "df_pos = df_raw[df_raw['positive']==1]\n",
    "str_all=[]\n",
    "def func(text):\n",
    "    str_all.append(text)\n",
    "df_pos.apply(lambda x: func(x['text']),axis=1)\n",
    "\n",
    "# learn on the first 10 smaples\n",
    "strrring = ''\n",
    "for i in range(10):\n",
    "    strrring+=str_all[i]\n",
    "\n",
    "print(strrring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(strrring))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in strrring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'q',\n",
       " 1: 'P',\n",
       " 2: '6',\n",
       " 3: '(',\n",
       " 4: 'O',\n",
       " 5: 'U',\n",
       " 6: 'e',\n",
       " 7: 'D',\n",
       " 8: '1',\n",
       " 9: '2',\n",
       " 10: 'S',\n",
       " 11: 'y',\n",
       " 12: '0',\n",
       " 13: '7',\n",
       " 14: 'o',\n",
       " 15: 'b',\n",
       " 16: '/',\n",
       " 17: 'R',\n",
       " 18: 'V',\n",
       " 19: '8',\n",
       " 20: 'E',\n",
       " 21: 'N',\n",
       " 22: 'c',\n",
       " 23: 'H',\n",
       " 24: '>',\n",
       " 25: \"'\",\n",
       " 26: 'A',\n",
       " 27: 'k',\n",
       " 28: 'F',\n",
       " 29: 'f',\n",
       " 30: ';',\n",
       " 31: '5',\n",
       " 32: 'K',\n",
       " 33: 'i',\n",
       " 34: 'a',\n",
       " 35: ':',\n",
       " 36: 'p',\n",
       " 37: ' ',\n",
       " 38: '.',\n",
       " 39: 'M',\n",
       " 40: 'l',\n",
       " 41: 'j',\n",
       " 42: 'h',\n",
       " 43: 'L',\n",
       " 44: 'v',\n",
       " 45: 't',\n",
       " 46: ')',\n",
       " 47: '\\x85',\n",
       " 48: 'n',\n",
       " 49: 'C',\n",
       " 50: '-',\n",
       " 51: 'g',\n",
       " 52: 'G',\n",
       " 53: 'd',\n",
       " 54: 'I',\n",
       " 55: '3',\n",
       " 56: '?',\n",
       " 57: 'W',\n",
       " 58: '\"',\n",
       " 59: 'T',\n",
       " 60: 'B',\n",
       " 61: 'r',\n",
       " 62: 'x',\n",
       " 63: 'm',\n",
       " 64: ',',\n",
       " 65: 'w',\n",
       " 66: '<',\n",
       " 67: 'z',\n",
       " 68: '!',\n",
       " 69: '~',\n",
       " 70: 'Y',\n",
       " 71: 'J',\n",
       " 72: 'u',\n",
       " 73: 's',\n",
       " 74: '*'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded.shape)\n",
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros([len(arr), n_labels], dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "#     one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "#     print(arr.flatten().tolist())\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten().int()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = torch.from_numpy(one_hot.reshape((*arr.shape, n_labels)))\n",
    "    \n",
    "    return one_hot\n",
    "# print(type(line_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = one_hot_encode(encoded, len(chars))\n",
    "# np.shape(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13606, 10]) torch.Size([13606])\n"
     ]
    }
   ],
   "source": [
    "def get_batches(arr, seq_length): \n",
    "    X=[]\n",
    "    y=[]\n",
    "    for ind in range(np.shape(arr)[0]-seq_length):\n",
    "        X.append(arr[ind:ind+seq_length]) #,:])\n",
    "        y.append(arr[ind+seq_length])# ,:])\n",
    "#     return np.moveaxis(np.array(X),0,1),np.array(y)\n",
    "    return torch.from_numpy(np.array(X).reshape(-1,seq_length)).float(), torch.from_numpy(np.array(y)).float()\n",
    "X,y = get_batches(encoded,10) #try without onehot.\n",
    "# y_cold = y.max(1)[1]\n",
    "print(np.shape(X),np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) #(129,128)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size) #(129,75)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "#         print('forward sizes',input.size(),hidden.size())\n",
    "        combined = torch.cat((inputs,hidden),1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# n_hidden = 128\n",
    "# n_letters = 1 #len(chars)*10\n",
    "# n_categories = len(chars)\n",
    "# rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# hidden =torch.zeros(1, n_hidden)\n",
    "# output, next_hidden = rnn.forward(X[0,0].reshape(1,1), hidden)\n",
    "# print(output,next_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m', 63)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return int2char[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13605"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ',\n",
       " 'olent than',\n",
       " tensor(37.),\n",
       " tensor([14., 40.,  6., 48., 45., 37., 45., 42., 34., 48.]))"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l, t):\n",
    "    randind = random.randint(0,np.shape(l)[0]-1)\n",
    "    return l[randind ,:], t[randind] #, tc[randind]\n",
    "def randomTrainingExample():\n",
    "    randX, randy = randomChoice(X,y) #,y_cold)\n",
    "#     print(randX)\n",
    "#     reshaped_str = np.reshape(randX,(10,-1),order='C')\n",
    "    trainstr = ''\n",
    "    train = [int2char[ii.item()] for ii in randX]\n",
    "    for ii in train: trainstr+=ii\n",
    "#     print('[',trainstr,'][',int2char[randy.item()],']')\n",
    "    return int2char[randy.item()], trainstr, randy, randX\n",
    "#     return trainstr, int2char[np.argmax(randy).item()], randX.reshape(1,-1), randy, randy_c\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "#     print(np.shape(line_tensor))\n",
    "    outputs = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i].reshape(1,-1), hidden)\n",
    "        outputs.append(output)\n",
    "    outputs = torch.stack(outputs)\n",
    "    \n",
    "    loss = criterion(output, category_tensor.reshape(1,).long()) #just use the final output to calculate the loss\n",
    "#     print('lossis', output, category_tensor)\n",
    "#     print(torch.min(torch.max(outputs,1)[1].reshape(-1,1).float()),torch.min(torch.max(category_tensor, 1)[1]))\n",
    "#     loss = criterion(torch.max(outputs,1)[1].reshape(-1,1).float(), category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(learning_rate, p.grad.data)\n",
    "#         print('post zero', p.grad.data)\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2% (0m 0s) 4.3195  big digge / ~ ✗ (r)\n",
      "200 4% (0m 0s) 4.4236  who has a / ( ✗ (d)\n",
      "300 6% (0m 0s) 4.4242 tling drea / 1 ✗ (d)\n",
      "400 8% (0m 0s) 4.3184 e film NOW / ~ ✗ (.)\n",
      "500 10% (0m 1s) 4.6458 omments, a / ( ✗ (n)\n",
      "600 12% (0m 1s) 4.6184 ave been f / B ✗ (a)\n",
      "700 14% (0m 1s) 5.2326 ead Maupin / B ✗ ( )\n",
      "800 16% (0m 1s) 5.7538 tor in the / P ✗ ( )\n",
      "900 18% (0m 1s) nan Blazing Sa / * ✗ (d)\n",
      "1000 20% (0m 2s) nan lt shrugs  / * ✗ (i)\n",
      "1100 22% (0m 2s) nan  Lisa Emer / * ✗ (y)\n",
      "1200 24% (0m 2s) nan  one physi / * ✗ (c)\n",
      "1300 26% (0m 2s) nan . Ms. Oh i / * ✗ (s)\n",
      "1400 28% (0m 2s) nan stic view  / * ✗ (o)\n",
      "1500 30% (0m 3s) nan it the nex / * ✗ (t)\n",
      "1600 32% (0m 3s) nan <br /><br  / * ✗ (/)\n",
      "1700 34% (0m 3s) nan ke a news  / * ✗ (r)\n",
      "1800 36% (0m 3s) nan shrugs ind / * ✗ (i)\n",
      "1900 38% (0m 4s) nan rm of a yo / * ✗ (u)\n",
      "2000 40% (0m 4s) nan ad million / * ✗ (a)\n",
      "2100 42% (0m 4s) nan  is correc / * ✗ (t)\n",
      "2200 44% (0m 4s) nan You really / * ✗ ( )\n",
      "2300 46% (0m 4s) nan n probably / * ✗ ( )\n",
      "2400 48% (0m 5s) nan ical thril / * ✗ (l)\n",
      "2500 50% (0m 5s) nan in the wor / * ✗ (l)\n",
      "2600 52% (0m 5s) nan  (who co-w / * ✗ (r)\n",
      "2700 54% (0m 5s) nan  in the st / * ✗ (r)\n",
      "2800 56% (0m 5s) nan kian suspe / * ✗ (n)\n",
      "2900 57% (0m 6s) nan saw this m / * ✗ (o)\n",
      "3000 60% (0m 6s) nan na and Pet / * ✗ (e)\n",
      "3100 62% (0m 6s) nan ch bombed  / * ✗ (w)\n",
      "3200 64% (0m 6s) nan el No one, / * ✗ ( )\n",
      "3300 66% (0m 6s) nan ISTENER (2 / * ✗ (0)\n",
      "3400 68% (0m 7s) nan situation, / * ✗ ( )\n",
      "3500 70% (0m 7s) nan and predic / * ✗ (t)\n",
      "3600 72% (0m 7s) nan performanc / * ✗ (e)\n",
      "3700 74% (0m 7s) nan n't seem,  / * ✗ (i)\n",
      "3800 76% (0m 7s) nan nely wants / * ✗ ( )\n",
      "3900 78% (0m 8s) nan watch the  / * ✗ (f)\n",
      "4000 80% (0m 8s) nan nflicted g / * ✗ (o)\n",
      "4100 82% (0m 8s) nan th a sissy / * ✗ ( )\n",
      "4200 84% (0m 8s) nan  slighter  / * ✗ (d)\n",
      "4300 86% (0m 9s) nan plot that  / * ✗ (w)\n",
      "4400 88% (0m 9s) nan oting hims / * ✗ (e)\n",
      "4500 90% (0m 9s) nan >The story / * ✗ ( )\n",
      "4600 92% (0m 9s) nan ight fare. / * ✗ (<)\n",
      "4700 94% (0m 9s) nan . It's inc / * ✗ (r)\n",
      "4800 96% (0m 10s) nan o see what / * ✗ ( )\n",
      "4900 98% (0m 10s) nan . I expect / * ✗ ( )\n",
      "5000 100% (0m 10s) nan new life,  / * ✗ (s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 5000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "learning_rate = 0.005\n",
    "\n",
    "# initiate model\n",
    "n_hidden = 32\n",
    "n_letters = 75 #len(chars)*10\n",
    "n_categories = len(chars)\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "# output, loss = train(y_cold, X)\n",
    "# print(timeSince(start))\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    line_tensor = one_hot_encode(line_tensor, n_categories)\n",
    "#     print(line_tensor)\n",
    "    output, loss = train(category_tensor.reshape(1,1), line_tensor)\n",
    "#     output, loss = train(y_cold, X)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.train(category_tensor, line_tensor)>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         print(sentence.size())\n",
    "        lstm_out, self.hidden = self.lstm(sentence, self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         print(tag_space.size())\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = lstm.init_hidden()\n",
    "    lstm.zero_grad()\n",
    "#     print(np.shape(line_tensor))\n",
    "#     outputs = []\n",
    "#     for i in range(line_tensor.size()[0]):\n",
    "    output = lstm.forward(line_tensor.view(1,1,-1))\n",
    "#         outputs.append(output)\n",
    "#     outputs = torch.stack(outputs)\n",
    "    \n",
    "    loss = criterion(output, category_tensor.reshape(1,).long()) #just use the final output to calculate the loss\n",
    "#     print('lossis', output, category_tensor)\n",
    "#     print(torch.min(torch.max(outputs,1)[1].reshape(-1,1).float()),torch.min(torch.max(category_tensor, 1)[1]))\n",
    "#     loss = criterion(torch.max(outputs,1)[1].reshape(-1,1).float(), category_tensor)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in lstm.parameters():\n",
    "#         p.data.add_(learning_rate, p.grad.data)\n",
    "#         print('post zero', p.grad.data)\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1% (0m 7s) 2.7795  too as th /   ✗ (e)\n",
      "200 2% (0m 23s) 5.9118 , Collette /   ✗ (')\n",
      "300 3% (0m 54s) 5.4376 t Listener /   ✗ (\")\n",
      "400 4% (1m 33s) 2.6753  his own r /   ✗ (e)\n",
      "500 5% (2m 24s) 3.9111 tinks\" to  /   ✗ (b)\n",
      "600 6% (3m 22s) 2.4977 ><br />Mel / d ✗ ( )\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 750\n",
    "HIDDEN_DIM = 128\n",
    "lstm = LSTM(EMBEDDING_DIM, HIDDEN_DIM, len(chars), len(chars))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "start = time.time()\n",
    "n_iters = 10000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "for iter in range(1, n_iters + 1):\n",
    "    lstm.zero_grad()\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    line_tensor = one_hot_encode(line_tensor, n_categories)\n",
    "#     print(line_tensor.size())\n",
    "    output, loss = train(category_tensor.reshape(1,1), line_tensor)\n",
    "#     output, loss = train(y_cold, X)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('word_embeddings.weight',\n",
       "              tensor([[ 1.4019,  0.7592, -0.1744,  ..., -1.0821,  1.7427, -1.8301],\n",
       "                      [ 0.2770, -1.2941,  1.1478,  ...,  0.3614, -1.0851, -0.6375],\n",
       "                      [ 0.1778, -0.5748,  0.7365,  ..., -0.3112, -0.3029, -1.8446],\n",
       "                      ...,\n",
       "                      [-0.7961, -1.4003,  0.5016,  ..., -0.6924,  0.9593,  0.2017],\n",
       "                      [-0.9170, -0.1393,  0.7344,  ...,  0.8253, -0.9071,  0.5072],\n",
       "                      [-1.1378, -0.0972, -0.3705,  ..., -0.0116,  2.0097, -0.4688]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 2.2874e-02,  8.6096e-02,  7.1945e-02,  ...,  5.2616e-02,\n",
       "                        2.2873e-02, -3.8136e-02],\n",
       "                      [-8.5802e-02, -3.6061e-02, -9.4667e-03,  ..., -6.1878e-02,\n",
       "                       -6.9095e-02,  4.1901e-02],\n",
       "                      [-8.5189e-02, -5.4993e-02,  8.4023e-02,  ...,  1.1696e-02,\n",
       "                       -7.9350e-02, -7.1315e-02],\n",
       "                      ...,\n",
       "                      [ 5.5136e-02, -7.3593e-02, -5.8487e-02,  ..., -3.1877e-02,\n",
       "                        5.0574e-02, -2.4596e-02],\n",
       "                      [-8.6813e-03, -2.1607e-02,  1.2478e-02,  ...,  5.4289e-02,\n",
       "                        2.0678e-02, -3.4803e-02],\n",
       "                      [ 7.7743e-02, -2.9316e-03,  4.1240e-02,  ..., -1.1951e-05,\n",
       "                        1.1423e-02, -5.0945e-02]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0726, -0.0571, -0.0302,  ..., -0.0589, -0.0497,  0.0926],\n",
       "                      [-0.0344, -0.0297,  0.0887,  ...,  0.0499, -0.0247, -0.0723],\n",
       "                      [ 0.0740,  0.0382, -0.0288,  ...,  0.0073, -0.0753, -0.0096],\n",
       "                      ...,\n",
       "                      [-0.0047,  0.0435,  0.0076,  ...,  0.0848, -0.0602, -0.0155],\n",
       "                      [-0.0722,  0.0123, -0.0352,  ..., -0.0081, -0.0083,  0.0454],\n",
       "                      [ 0.0425,  0.0466,  0.0490,  ..., -0.0780, -0.0184,  0.0739]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 7.4524e-03,  1.1912e-01,  9.9756e-02,  4.4134e-02,  2.0057e-01,\n",
       "                       5.2237e-02,  8.3696e-02,  1.0955e-01,  5.3393e-02,  1.5464e-01,\n",
       "                      -1.5015e-02,  1.2717e-01,  5.0602e-02,  5.2722e-03,  8.2886e-02,\n",
       "                       6.1791e-02, -3.7517e-02,  9.5228e-02, -4.2724e-03, -1.8538e-02,\n",
       "                      -6.6499e-02,  4.2961e-02, -4.4948e-02,  5.3679e-03, -4.6082e-02,\n",
       "                       2.1670e-02,  2.1435e-02,  9.0372e-03,  3.7271e-02,  9.3892e-02,\n",
       "                       2.3756e-01,  9.3402e-02,  8.6176e-02,  1.3627e-01,  6.0408e-02,\n",
       "                       9.9534e-02, -2.1244e-02,  3.4343e-02,  1.0225e-02,  1.0763e-01,\n",
       "                       1.3891e-03, -2.1147e-02,  8.4363e-02,  2.7017e-02, -3.4586e-03,\n",
       "                       9.7851e-02,  1.5178e-02, -1.6109e-03,  8.6823e-02,  1.9171e-01,\n",
       "                       8.2695e-02,  4.2328e-02,  1.4739e-01,  1.1201e-01, -2.2200e-02,\n",
       "                       1.5577e-03,  1.4240e-01,  5.8354e-02,  8.5885e-02,  5.3610e-02,\n",
       "                       1.2071e-01,  1.7416e-01, -3.6120e-02,  1.0699e-01,  3.4833e-01,\n",
       "                       1.9459e-02, -3.3707e-02,  2.1807e-01,  1.6404e-01,  4.7117e-02,\n",
       "                       2.9005e-02,  2.5215e-01, -1.1375e-02,  1.5204e-02, -7.2073e-02,\n",
       "                       1.5088e-01, -5.5727e-03,  1.1600e-01,  5.5534e-02,  8.3640e-02,\n",
       "                      -2.2536e-02,  8.8409e-02,  1.3537e-02,  1.6513e-01,  3.2264e-02,\n",
       "                       3.2356e-01,  1.7263e-01,  9.2262e-02,  1.3000e-01, -2.2418e-02,\n",
       "                       6.5674e-02,  1.1993e-01,  6.9406e-02, -6.3114e-02,  6.0821e-02,\n",
       "                      -7.1784e-03,  6.7091e-02, -3.0471e-02,  2.0828e-02, -1.5504e-02,\n",
       "                       2.0056e-02,  8.1843e-03, -9.2110e-02,  2.5524e-02,  5.0896e-02,\n",
       "                       1.2470e-01, -6.4548e-02,  2.5629e-02, -3.8886e-02,  7.5007e-03,\n",
       "                       1.5655e-01,  6.2673e-02,  1.7969e-03,  4.4559e-02,  5.4793e-02,\n",
       "                       9.9003e-02, -3.7172e-02,  6.6518e-02, -3.4242e-02,  1.0165e-02,\n",
       "                       4.6004e-02, -1.5786e-02,  1.1560e-01,  7.2196e-02,  3.7925e-01,\n",
       "                       2.9035e-02, -3.8961e-02,  4.8833e-02,  2.0735e-02, -1.2710e-01,\n",
       "                      -1.3390e-03, -6.9781e-02, -1.9951e-01, -2.8713e-02, -1.0301e-01,\n",
       "                      -9.8439e-02,  8.5573e-03, -1.5887e-01, -5.3961e-03, -1.0903e-01,\n",
       "                      -5.3943e-02,  2.7268e-02, -9.6554e-02, -1.0355e-01, -1.1572e-01,\n",
       "                      -4.2619e-02, -1.8464e-01, -1.2665e-01, -1.0204e-01, -1.7580e-01,\n",
       "                      -9.3535e-02, -8.8978e-02, -9.1389e-02,  3.4838e-03, -1.6471e-02,\n",
       "                      -1.3691e-01, -1.3241e-01, -7.4788e-02, -7.7917e-02, -8.4700e-03,\n",
       "                      -4.6950e-03, -2.0765e-01, -1.4861e-01, -1.2085e-01, -9.6298e-02,\n",
       "                      -1.4033e-01,  2.7292e-02, -5.2438e-02, -1.1426e-01, -4.0750e-02,\n",
       "                      -4.3560e-02, -8.5947e-02, -2.3905e-02, -1.1991e-01, -2.4750e-02,\n",
       "                      -3.7519e-03,  9.6530e-03, -1.5780e-01, -6.2527e-02, -1.2765e-01,\n",
       "                      -7.9208e-02, -1.3618e-01, -7.6122e-02,  1.1442e-02, -1.7314e-01,\n",
       "                      -1.0701e-01, -6.9815e-02, -6.8213e-02, -1.8051e-02, -8.5766e-02,\n",
       "                      -7.5667e-02, -6.8734e-02, -2.2140e-01, -3.9798e-02, -7.7369e-02,\n",
       "                      -1.8440e-01, -1.2837e-01, -7.1522e-02, -9.3213e-02, -6.0492e-02,\n",
       "                      -4.8081e-02,  3.1882e-03, -6.5579e-03, -1.2253e-01, -1.2048e-01,\n",
       "                      -3.3613e-02, -9.9415e-02,  1.8185e-02, -9.3043e-02, -1.6030e-01,\n",
       "                      -3.7278e-02, -4.1166e-02, -5.6276e-03, -1.6912e-01, -6.7919e-02,\n",
       "                      -1.2938e-01, -1.1151e-01, -3.4544e-02, -1.0558e-01, -1.1313e-01,\n",
       "                      -2.6039e-02, -9.7144e-02, -8.3910e-03, -1.4180e-01,  1.0844e-04,\n",
       "                      -5.1977e-02, -4.1563e-02, -1.2344e-01,  2.2645e-02, -1.3094e-01,\n",
       "                       1.0277e-02, -1.1871e-01, -9.8741e-02,  6.6302e-03, -1.3724e-01,\n",
       "                       1.0542e-02, -7.1696e-02, -8.6057e-02, -1.4696e-01, -5.7510e-02,\n",
       "                      -5.9378e-02,  2.3183e-02, -1.1433e-01, -7.2347e-02, -8.0864e-02,\n",
       "                       6.9436e-04, -1.3098e-01, -1.3424e-01,  2.8980e-02,  2.6817e-02,\n",
       "                      -1.4538e-01, -6.2467e-02, -2.8690e-01, -6.7057e-03, -1.5262e-01,\n",
       "                      -2.0561e-02,  2.1051e-01,  1.0322e-01, -5.6171e-02, -2.9195e-01,\n",
       "                      -4.0448e-02, -1.2633e-01,  7.0912e-02, -4.7649e-02, -3.1135e-02,\n",
       "                      -9.0770e-02,  1.5858e-01,  1.8347e-02,  8.5341e-02, -3.3001e-02,\n",
       "                      -1.9314e-02,  1.5873e-02,  1.0298e-01,  6.4694e-02,  4.6672e-02,\n",
       "                       1.3715e-01,  1.1561e-01,  1.1016e-01, -4.4135e-02,  7.9749e-02,\n",
       "                       1.9118e-02, -6.4544e-02, -1.5184e-01, -3.8867e-02,  1.2311e-02,\n",
       "                      -5.7326e-02, -1.7404e-02, -9.5540e-02,  7.7401e-02, -5.7467e-02,\n",
       "                      -8.8246e-02,  1.9033e-02, -3.8609e-03, -1.1390e-01, -6.8812e-02,\n",
       "                       2.6729e-02, -7.3726e-03, -3.5718e-02,  4.1272e-02, -3.3600e-02,\n",
       "                       3.4703e-02,  1.1061e-01, -4.4916e-02,  5.3247e-02, -8.2108e-02,\n",
       "                       1.2232e-02, -1.5343e-02,  5.2208e-02, -5.7016e-02, -7.6559e-02,\n",
       "                       1.8984e-02,  2.9777e-02,  6.3246e-03,  5.9765e-02,  6.5627e-02,\n",
       "                      -3.7374e-02,  1.3121e-01, -1.5337e-01, -8.1542e-02, -7.9216e-02,\n",
       "                      -8.3266e-02,  2.8474e-02, -7.5967e-02, -5.9160e-02, -5.0380e-02,\n",
       "                      -3.3552e-03, -1.9061e-02, -5.9217e-02,  1.3674e-01,  1.3897e-01,\n",
       "                      -8.1749e-02, -5.9153e-02,  8.0573e-02, -3.3614e-02,  1.6344e-02,\n",
       "                      -1.1564e-01,  1.5319e-01, -1.0662e-02,  9.6068e-02, -1.4766e-02,\n",
       "                       9.4434e-02, -4.6385e-03,  5.5755e-02,  9.5142e-02,  1.4483e-01,\n",
       "                       1.5486e-02, -1.0865e-01,  4.4864e-02, -1.1172e-02, -1.5991e-01,\n",
       "                      -1.2943e-02, -2.7443e-02, -3.1327e-02, -1.5998e-02, -4.0150e-02,\n",
       "                       5.4270e-02,  8.9711e-02,  2.6376e-02,  1.7627e-02, -6.4569e-02,\n",
       "                      -1.4477e-01, -1.6851e-02, -2.1934e-02,  8.3499e-02,  4.2649e-02,\n",
       "                      -4.5908e-02, -4.8834e-02, -3.7921e-02, -6.1995e-02,  6.6354e-03,\n",
       "                       1.0544e-01, -6.4495e-02,  6.0625e-02,  1.9994e-01,  6.2627e-02,\n",
       "                       1.0569e-02, -2.6368e-01, -1.8041e-02,  2.4566e-03, -4.2141e-02,\n",
       "                       1.9313e-01,  6.8530e-02, -1.3228e-01, -5.8133e-02, -5.4356e-02,\n",
       "                       8.1994e-02,  1.3656e-01, -1.1262e-01,  1.7111e-01, -8.8170e-02,\n",
       "                       1.5351e-01,  1.1782e-01, -4.1870e-02,  8.5717e-02, -3.3121e-02,\n",
       "                       1.3244e-01, -1.1215e-02,  5.4253e-02,  1.1597e-01,  9.1255e-02,\n",
       "                       9.0864e-02, -2.6384e-02,  1.0922e-01,  1.8275e-02, -4.1984e-02,\n",
       "                       2.3127e-01, -2.2395e-02,  5.0816e-02,  1.0124e-01,  1.2094e-01,\n",
       "                       8.2892e-03, -5.3884e-02,  1.1937e-02,  3.6631e-02,  1.7809e-01,\n",
       "                      -1.1117e-02,  1.3413e-01,  2.9547e-01, -3.4146e-02,  1.0857e-01,\n",
       "                      -2.6689e-03, -3.1961e-04, -7.5189e-02, -4.1617e-02, -5.3131e-02,\n",
       "                       1.4440e-02,  9.8838e-02,  1.4914e-02, -7.4313e-02,  7.3263e-02,\n",
       "                      -5.8866e-02, -6.9978e-02,  1.6687e-02,  2.4514e-01, -5.8450e-02,\n",
       "                       5.9146e-02,  6.8468e-02,  1.7082e-01, -9.9682e-02,  6.9693e-02,\n",
       "                       2.3801e-01,  1.1252e-01,  1.8692e-02, -4.8823e-02,  3.8664e-02,\n",
       "                       2.6002e-01, -5.2271e-02,  1.5225e-02,  3.5776e-01,  3.6810e-02,\n",
       "                       5.4377e-02,  2.6150e-01,  7.5068e-02, -3.3953e-02,  8.1840e-02,\n",
       "                       1.0338e-01,  3.4724e-02, -9.4761e-02,  6.4575e-02,  1.1496e-01,\n",
       "                       3.5515e-02,  6.9612e-02, -2.9272e-02, -6.8165e-03,  6.6753e-02,\n",
       "                       1.1812e-01,  3.5720e-02,  9.1059e-02, -4.6426e-02,  2.7620e-01,\n",
       "                       3.0422e-02,  6.4557e-02,  1.8071e-01, -1.9341e-02,  1.9727e-01,\n",
       "                      -3.7660e-02,  2.7458e-03, -1.0038e-01,  8.5501e-02,  4.2118e-02,\n",
       "                      -4.8154e-02,  6.8692e-02,  5.3781e-02,  7.3635e-02,  9.8500e-02,\n",
       "                       3.0453e-02,  6.5327e-02, -4.9457e-02,  1.7824e-01,  1.5277e-02,\n",
       "                      -1.2981e-02,  3.0079e-02, -8.8644e-03,  1.0959e-02,  1.1427e-01,\n",
       "                      -3.4614e-02,  1.9107e-03,  1.2408e-01,  7.4798e-02,  7.6533e-02,\n",
       "                      -5.4112e-02, -4.9618e-02,  8.6760e-02,  1.4902e-01, -1.8841e-01,\n",
       "                      -7.3697e-02,  1.4091e-01,  6.0935e-03,  4.1019e-01, -6.5040e-03,\n",
       "                       5.2792e-02,  1.8714e-02])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 7.3127e-02,  1.0770e-01,  1.0652e-01, -7.1379e-02,  8.3315e-02,\n",
       "                      -1.2119e-02,  2.1630e-01,  1.1268e-02,  3.2499e-02,  8.8869e-02,\n",
       "                      -1.9298e-02,  1.6862e-01,  9.0453e-02,  7.7732e-02,  1.5950e-01,\n",
       "                       1.4719e-01,  4.8207e-02,  2.7789e-03,  1.3564e-01,  9.3380e-02,\n",
       "                      -1.2236e-02,  1.4394e-01,  4.7390e-02,  1.0006e-01, -2.2099e-02,\n",
       "                       8.3966e-02, -2.1188e-02, -6.9124e-02, -1.4953e-02,  8.6783e-02,\n",
       "                       1.9708e-01, -9.0182e-03, -1.4433e-02,  2.2435e-01, -2.7363e-02,\n",
       "                       1.2208e-01, -1.3702e-02, -2.7522e-02,  4.5155e-02,  6.4744e-02,\n",
       "                       3.6851e-02,  1.0866e-02, -3.1703e-02,  1.3353e-02, -5.7686e-02,\n",
       "                       6.7728e-02, -5.9548e-02, -5.1759e-02,  1.1758e-01,  1.0526e-01,\n",
       "                      -1.9918e-03,  1.0608e-02,  1.7746e-01,  1.2584e-01, -8.2649e-02,\n",
       "                       8.0245e-02,  2.1708e-01,  4.8616e-02,  7.3596e-02, -6.3204e-02,\n",
       "                       5.2249e-02,  1.5123e-01, -6.7163e-02,  5.0667e-02,  3.9867e-01,\n",
       "                       3.2793e-02,  9.8155e-03,  1.1968e-01,  1.6456e-01, -2.7313e-02,\n",
       "                       1.6637e-02,  1.1167e-01,  8.2919e-02,  9.2507e-03,  3.4904e-02,\n",
       "                       5.1997e-02,  2.2055e-02,  3.6397e-02, -4.4466e-02, -5.0072e-02,\n",
       "                      -2.0938e-02,  8.4631e-02, -1.1774e-02,  1.0139e-01, -2.6616e-02,\n",
       "                       2.4544e-01,  7.8337e-02,  1.1157e-01,  9.9071e-02,  3.9409e-02,\n",
       "                       1.1882e-01,  8.0098e-02,  1.2824e-02,  1.2588e-02,  2.2745e-02,\n",
       "                       1.4488e-01, -6.8819e-02, -6.7263e-02,  6.3204e-02, -6.1848e-02,\n",
       "                       4.5485e-02,  6.5784e-02,  7.6942e-02,  9.7217e-02,  4.8691e-02,\n",
       "                       8.7141e-02, -5.2504e-02, -4.8175e-02,  7.9246e-02,  5.5765e-02,\n",
       "                       7.9056e-03,  3.3346e-02,  2.6991e-02, -6.3949e-03,  1.2538e-01,\n",
       "                       4.9025e-03,  6.8986e-03,  5.1638e-06,  2.7862e-02,  8.0867e-02,\n",
       "                       8.4166e-02, -9.1499e-02,  1.1353e-01,  1.8573e-02,  3.7024e-01,\n",
       "                      -3.6414e-02,  4.0292e-02,  6.4476e-02,  4.1086e-02, -1.0326e-01,\n",
       "                      -1.5353e-01, -1.4035e-02, -1.8026e-01, -9.4310e-02, -5.2397e-02,\n",
       "                      -9.6472e-02, -4.9808e-02, -1.4440e-01,  7.7413e-03, -6.5094e-02,\n",
       "                       1.5760e-02, -1.3639e-01, -9.3767e-02, -6.4560e-02,  2.2825e-02,\n",
       "                      -1.2140e-01, -7.5736e-02, -1.2525e-01, -9.5743e-02, -3.7698e-02,\n",
       "                      -7.9691e-02, -1.4255e-01, -2.1963e-02, -9.7310e-02, -1.2274e-01,\n",
       "                       8.2127e-04, -3.0210e-02, -1.5727e-01, -1.7350e-01, -1.5132e-01,\n",
       "                      -1.2766e-01, -2.1003e-01, -2.4041e-02, -1.1851e-01, -7.3872e-02,\n",
       "                      -7.4980e-02,  4.5558e-02, -8.8527e-03, -5.9249e-02, -4.2783e-02,\n",
       "                      -6.3426e-02, -1.4622e-01, -3.1300e-02, -5.1775e-02, -1.6082e-02,\n",
       "                      -8.7095e-02, -2.6250e-03, -2.0458e-01, -6.3757e-02, -6.6077e-02,\n",
       "                      -1.7937e-01, -3.7420e-02, -3.7691e-02, -1.3399e-01, -1.3670e-01,\n",
       "                      -7.8883e-02, -7.3000e-02,  2.2808e-02, -1.1054e-01, -1.1438e-01,\n",
       "                      -4.0092e-02, -2.4998e-02, -2.1721e-01,  3.6174e-02, -3.8744e-02,\n",
       "                      -1.3321e-01, -1.4371e-01, -7.1984e-02, -1.4889e-03, -1.1032e-01,\n",
       "                      -1.2105e-01,  1.1497e-02, -7.3101e-02, -2.1333e-02, -3.3545e-02,\n",
       "                      -7.8807e-02, -6.7642e-02, -7.7983e-02, -9.5476e-03, -1.2313e-01,\n",
       "                      -8.8868e-02, -3.9575e-02,  3.4779e-02, -2.3306e-01, -1.4453e-03,\n",
       "                      -9.7995e-02, -1.4611e-01, -2.6496e-02, -7.3055e-03, -8.4912e-02,\n",
       "                      -2.4550e-02, -5.1803e-02, -2.5672e-02, -3.1133e-02,  7.1119e-03,\n",
       "                      -9.1244e-02,  3.4039e-02, -2.0130e-02, -3.7511e-02, -8.2956e-02,\n",
       "                       1.1933e-02,  2.8569e-02, -2.3010e-02, -1.2387e-01, -5.7658e-02,\n",
       "                       1.4616e-02, -4.1126e-02, -7.9652e-02, -5.2228e-02,  1.3186e-02,\n",
       "                       1.7975e-02, -1.1519e-01, -1.5715e-01, -1.0827e-01, -2.6447e-02,\n",
       "                      -4.4023e-02, -1.2546e-01, -1.1178e-01,  3.5156e-02, -1.1764e-01,\n",
       "                      -4.5628e-02, -1.4212e-01, -2.9215e-01, -6.1729e-03, -1.0632e-01,\n",
       "                      -1.2896e-01,  2.0724e-01,  1.3238e-03, -6.4090e-02, -2.5744e-01,\n",
       "                      -1.1393e-01, -5.8576e-02,  5.4606e-02, -2.7995e-02, -5.7368e-02,\n",
       "                      -1.2737e-01,  3.0582e-02,  9.2799e-02,  1.8087e-01, -1.3270e-02,\n",
       "                       1.7974e-02, -2.4507e-02,  2.6504e-02,  1.9594e-02, -2.7924e-02,\n",
       "                       7.5013e-02,  1.2509e-01,  1.5989e-01,  1.0455e-02,  7.9241e-02,\n",
       "                      -2.9037e-02, -3.2857e-02, -2.4639e-02, -1.2213e-01, -3.2479e-02,\n",
       "                      -8.8727e-02,  1.0376e-01, -5.6614e-02,  7.1979e-02, -4.7337e-02,\n",
       "                      -5.5603e-02, -4.7309e-02, -1.4261e-02, -8.9665e-02,  3.5337e-02,\n",
       "                      -4.1396e-02, -1.4775e-02, -4.3549e-02, -3.9380e-02,  8.8795e-02,\n",
       "                      -2.0071e-02, -3.9015e-02,  4.1983e-02,  1.6290e-02,  1.0435e-02,\n",
       "                       9.2265e-02, -3.2490e-02, -1.5018e-02, -9.5512e-02, -2.0027e-02,\n",
       "                       3.3992e-02, -2.2249e-02, -2.4319e-02, -6.4662e-02,  6.4150e-02,\n",
       "                       3.4430e-03,  1.1308e-01, -2.6353e-02, -8.5196e-02, -7.1661e-02,\n",
       "                      -1.4123e-02, -1.3180e-02, -2.1542e-03,  3.5643e-02, -6.9213e-02,\n",
       "                      -6.0617e-02, -6.7933e-02,  7.7783e-02,  9.6637e-02,  7.0951e-02,\n",
       "                       4.9638e-03,  2.3352e-02,  1.6985e-02, -1.2453e-01,  2.5931e-02,\n",
       "                      -1.6639e-02,  9.1116e-02, -6.5885e-02,  1.8463e-02, -7.9722e-02,\n",
       "                       3.0755e-02,  1.4815e-03, -7.5944e-02,  1.7349e-02,  1.8236e-02,\n",
       "                      -7.8988e-02,  2.7991e-02, -4.4276e-02, -2.8416e-02, -1.6722e-02,\n",
       "                       4.3874e-02, -4.6691e-02,  1.0240e-01,  4.4081e-02,  1.7350e-02,\n",
       "                      -6.8917e-04,  9.2795e-03, -5.8687e-02,  7.1350e-02, -3.6615e-02,\n",
       "                       5.8360e-03, -6.3148e-02, -3.3213e-02,  8.6953e-02,  1.0499e-01,\n",
       "                       4.2903e-02, -1.7217e-02,  4.8084e-03, -4.2535e-02, -2.5718e-02,\n",
       "                       6.3713e-02,  6.6681e-02, -5.7847e-02,  2.0882e-01, -2.8637e-02,\n",
       "                       6.8738e-02, -3.2154e-01,  4.1234e-02,  5.2156e-02, -2.3958e-02,\n",
       "                       1.0482e-01, -3.3161e-03, -5.0242e-02,  4.2789e-02, -1.4613e-01,\n",
       "                       1.1555e-01,  7.6968e-02, -7.4408e-02,  1.5003e-01, -7.4105e-02,\n",
       "                       1.2548e-01,  6.7857e-03,  1.5702e-02,  2.9116e-03,  4.8468e-02,\n",
       "                       1.1357e-01, -1.1187e-01, -5.5554e-02,  3.4639e-02,  1.3111e-01,\n",
       "                       5.8193e-02, -6.0867e-03,  5.5800e-02, -4.3747e-02, -3.0672e-02,\n",
       "                       7.0376e-02,  3.1065e-02,  4.7203e-02,  7.5715e-02,  6.9674e-02,\n",
       "                      -7.1708e-02,  3.1929e-02,  1.0889e-01,  7.3365e-02,  1.3674e-01,\n",
       "                       1.2797e-02,  4.5206e-02,  2.5584e-01,  7.2409e-03,  8.6410e-02,\n",
       "                      -6.0421e-02,  3.7447e-02, -3.4315e-02,  5.1024e-03,  1.4528e-02,\n",
       "                      -1.4523e-02,  5.0083e-02,  3.7139e-02, -5.4252e-02,  1.0157e-01,\n",
       "                      -3.9917e-02, -5.0585e-02, -5.0738e-02,  1.8298e-01,  4.2889e-02,\n",
       "                       1.4781e-02,  1.8785e-01,  1.7592e-01, -1.0678e-02,  9.6483e-02,\n",
       "                       1.2733e-01,  7.7231e-02,  4.9385e-02,  4.1038e-02,  1.4371e-01,\n",
       "                       1.4344e-01, -7.0342e-02,  8.9250e-02,  4.0311e-01, -6.7295e-02,\n",
       "                      -3.6475e-02,  1.3290e-01,  1.6796e-01, -1.9102e-03, -3.6535e-02,\n",
       "                       2.6314e-01, -1.8936e-02, -3.8817e-02, -1.0155e-01,  4.1635e-02,\n",
       "                       6.9181e-02,  7.2168e-02,  9.0326e-04, -1.4116e-02, -2.6841e-02,\n",
       "                       5.1106e-02, -4.3019e-02,  1.6174e-01, -1.4502e-01,  2.5389e-01,\n",
       "                       2.1761e-02,  4.5953e-03,  1.1183e-01,  1.5546e-02,  4.4832e-02,\n",
       "                       1.1050e-01, -9.4341e-02, -1.3438e-03, -4.1425e-02,  2.7121e-02,\n",
       "                      -6.1982e-02, -4.9881e-02,  8.5955e-02,  4.8219e-02,  4.8961e-02,\n",
       "                       1.4159e-01, -2.4703e-02,  3.6002e-02,  4.7367e-02,  9.3144e-02,\n",
       "                      -1.5433e-01, -8.9578e-02,  6.9931e-02,  5.2506e-02, -1.2521e-02,\n",
       "                      -7.6645e-02, -6.3216e-02,  3.3624e-02,  2.8675e-02,  6.5284e-02,\n",
       "                       7.8037e-02, -6.9434e-02,  9.8996e-02,  2.2244e-02, -2.4362e-01,\n",
       "                       4.8514e-02,  3.3059e-02,  3.0448e-02,  4.0831e-01,  5.2440e-02,\n",
       "                       5.4504e-03, -6.6640e-02])),\n",
       "             ('hidden2tag.weight',\n",
       "              tensor([[-0.2349, -0.0320,  0.0866,  ...,  0.0076,  0.1765,  0.0314],\n",
       "                      [-0.1189, -0.1140,  0.0044,  ..., -0.1008,  0.0117, -0.0099],\n",
       "                      [-0.2253,  0.0539, -0.0141,  ..., -0.0707,  0.1865, -0.0203],\n",
       "                      ...,\n",
       "                      [ 0.3463, -0.0342, -0.1393,  ...,  0.0480, -0.1109, -0.0174],\n",
       "                      [ 0.1977,  0.0575,  0.0896,  ..., -0.1350, -0.4446, -0.0295],\n",
       "                      [-0.1064, -0.0157, -0.0410,  ..., -0.0186,  0.0882, -0.0366]])),\n",
       "             ('hidden2tag.bias',\n",
       "              tensor([-0.6860, -0.5363, -0.7495, -0.2521, -0.3056, -0.7542,  1.1545, -0.6472,\n",
       "                      -0.8010, -0.7205,  0.1425,  0.9030, -0.5806, -0.7118,  1.3613,  0.6704,\n",
       "                       0.2352, -0.5790, -0.7564, -0.8256, -0.6552, -0.4481,  0.9233, -0.4807,\n",
       "                       0.1842, -0.2032, -0.6848,  0.1763, -0.5770,  1.2548, -0.6828, -0.7456,\n",
       "                      -0.7763,  1.1416,  1.7322, -0.7034,  0.5373,  1.4695,  0.0926, -0.4836,\n",
       "                       1.2980, -0.6983,  1.1471, -0.5292,  0.3279,  1.7520, -0.3420, -0.7171,\n",
       "                       0.9929, -0.2346, -0.5209,  0.2136, -0.5885,  1.1236,  0.0783, -0.6572,\n",
       "                      -0.8024, -0.0870, -0.0057,  0.1976, -0.0723,  1.0478, -0.2665,  1.2408,\n",
       "                       0.6726,  0.4913,  0.2110, -0.8381, -0.7314, -0.7344, -0.6471, -0.6081,\n",
       "                       0.8451,  1.6013, -0.4253]))])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = lstm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "torch.Size([1, 75])\n",
      "tensor(1.0000)\n",
      "hello how are y.H6qjR<waiBfTn>07,Ch/PvP 22!R0CRUbv>WpKzAM*qDGb1.kIn/Bg7It7sBCo.;Ic",
      " C5p7VlEuoI7?pH3m20*zarWG?Mpdmd~\n"
     ]
    }
   ],
   "source": [
    "def randomFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return int2char[category_i], category_i\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=75):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size ALPHASIZE with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(np.exp(probabilities.detach().numpy()))\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(75, 1, p=p)[0]\n",
    "\n",
    "line = 'hello how are y'\n",
    "\n",
    "for ii in range(100):\n",
    "    linenum = [char2int[jj] for jj in line]\n",
    "    inline = one_hot_encode(torch.from_numpy(np.array(linenum[-10:])),75).view(1,1,-1)\n",
    "#     print(nn.Softmax(lstm.forward(inline)))\n",
    "    line += int2char[sample_from_probabilities(lstm.forward(inline))][0]\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_probabilities(lstm.forward(inline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;?!1234567890'\"\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "seq_len = 10 #read 10 chars at a time\n",
    "n_hidden = 128 #hidden layer\n",
    "n_letters = len(all_letters)\n",
    "n_categories = n_categories\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "input = lineToTensor('Albertddfw')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "Once the model is trained, we can use it to generate completely new text in the style of your training data.  **Train a model using your original choice of corpus below, and generate some sample sentences.** Don't worry too much about your loss / accuracy during training, but instead check on the text your model is generating. Your generated text should be somewhat coherent, i.e. similar to your training text in structure, and not excessively mispelled.\n",
    "\n",
    "An **example model architecture** is as follows (feel free to play around with different structures):\n",
    "* Embedding (for each character in your vocab) of dimension 64\n",
    "* Dropout of 20% for the embedding input to the RNN\n",
    "* 2 LSTM layers, each of dimension 512 (play around with the number and dimension of hidden layers)\n",
    "* Dropout of 50% for each LSTM layer\n",
    "* Dense softmax layer of same dimension as your vocab size (e.g. if your vocab size is 100, this layer is the probabilty that your output is one of 100 possible characters)\n",
    "    \n",
    "**You should understand what each of the above elements are and how they work at a high level by the end of this week's exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing the exercise\n",
    "How do you think you can apply what you learned in the above exercise to other problems involving text? For example, how would you tackle the previous IMDB sentiment classification task using an RNN architecture? **Discuss below.**\n",
    "\n",
    "(*Bonus*: create an RNN model for the IMDB classification task and discuss your results. How does the performance compare to your bag of words model?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: RNNs from scratch\n",
    "Now that you understand how to use RNNs, it's time to build a basic one from scratch.  You won't understand how they work until you get stuck in the weeds! \n",
    "\n",
    "### Generate text (Version 2)\n",
    "Your task is now to **build the forward pass of a simple RNN, without using any existing RNN APIs**. You can use PyTorch or Tensorflow (Keras is too high level for this exercise), both of which will automatically handle backpropagation for you.  If you use Tensorflow, please research and use Eager execution - it replaces Tensorflow's default graph / session framework, which is very difficult to learn and debug.\n",
    "\n",
    "Similar to last week's exercise, create a class for your network (write forward and loss steps, allowing PyTorch or Tensorflow to handle backpropagation for you).  Consider appropriate sizes for your input, hidden and output layers - your __init__ method should take in the params `hidden_size`, `vocab_size`, and `embedding_size` (if you use embeddings). Using these variables, you should initialise three weight layers `input_layer`, `hidden_layer`, and `output_layer`.  In an RNN, you will also have to deal with another item - the `hidden_state`. (Note: your RNN structure may vary slightly from this depending on your learning materials, but the key part is always `hidden_state`)\n",
    "\n",
    "You should **train your RNN on the same data and task as in Chapter 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the results of your basic RNN compare to your model in Chapter 3?**  What do you think explains the difference in performance? Discuss below.\n",
    "\n",
    "Some relevant resources on LSTMs (and RNN theory) below if you are interested:\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://www.youtube.com/watch?v=93rzMHtYT_0&list=LLpNVCNE9cYqVrjb2O8bZUGg&index=2&t=0s\n",
    "* https://www.youtube.com/watch?v=zQxm3Upr3_I\n",
    "* http://harinisuresh.com/2016/10/09/lstms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenges (not required!):\n",
    "1. Build the forward pass of an LSTM, without using any existing RNN APIs (as above, with PyTorch or Tensorflow)\n",
    "1. Build a basic RNN or LSTM in Numpy - including forward pass as well as backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
